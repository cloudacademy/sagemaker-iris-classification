{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9705aa3f",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Jupyter notebooks are divided into cells that can contain markdown or code that you can run interactively from the notebook interface. You can progress through the cells in the notebook by clicking the play button in the notebook tab's toolbar:\n",
    "\n",
    "![](assets/2024-09-06-10-35-01.png)\n",
    "\n",
    "Click the play button to advance to the next cell and continue on in the lab whenever you have completed a cell.\n",
    "\n",
    "After clicking the play button, the status in the left-hand side of the bottom status bar will change from **Idle** to **Busy**:\n",
    "\n",
    "![](assets/2024-09-06-10-46-24.png)\n",
    "\n",
    "Wait for the status to change back to **Idle** before proceeding to the next cell.\n",
    "\n",
    "#### Notebook Overview\n",
    "\n",
    "This notebook walks through the process for preparing data and then training a model using Amazon SageMaker HyperParameter Tuning.\n",
    "\n",
    "The dataset you will be using is the [public domain Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), which is a popular dataset for demonstrating classification with machine learning. The dataset consists of 150 samples of iris flowers. Using the numeric features of the flowers, such as sepal length and width, you will train a model that can classify each flower into one of three of the Iris flower species: setosa, versicolor, or virginica.\n",
    "\n",
    "You will use Python 3 as the programming language. This notebook is built on the SageMaker notebook conda_python3 environment. conda refers to Anaconda which is a data science platform. The environment comes with many common Python machine learning and data science libraries already installed.\n",
    "\n",
    "### Installing Dependencies\n",
    "\n",
    "To begin, you will ensure that you have the versions of the dependencies required by this lab. Run the following cell to install the versions of the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f537ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip install boto3==1.35.4 sagemaker==2.229.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417a28bd",
   "metadata": {},
   "source": [
    "### Importing the Required Libraries\n",
    "\n",
    "You are using the popular `numpy` and `pandas` libraries for data manipulation, along with `boto3` and `sagemaker` for interacting with Amazon SageMaker and Amazon S3 resources. Run the following cell to import them and create a SageMaker client session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81245351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "import numpy as np                                      # For performing matrix operations and numerical processing\n",
    "import pandas as pd                                     # For manipulating tabular data\n",
    "from sklearn.model_selection import train_test_split    # For splitting the data into training and testing sets\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "smclient = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e365d37",
   "metadata": {},
   "source": [
    "### Getting the Notebook's IAM Role\n",
    "\n",
    "To interact with SageMaker, you need an IAM role that grants the necessary permissions. A role was created for you during lab setup that grants access to Amazon S3 and to create and run Amazon SageMaker HyperParameter Tuning jobs.\n",
    "\n",
    "Run the following cell to get the role associated with the notebook instance and store it in a variable for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5fd80",
   "metadata": {},
   "source": [
    "### Configuring Storage\n",
    "\n",
    "To train a model, you need to access a dataset, prepare the data for training, and store the model artifacts. Using Amazon S3 for storage is a best practice when working with Amazon SageMaker.\n",
    "\n",
    "The following cell retrieves the name of a bucket beginning with `lab-notebook-` that was created for you during lab setup.\n",
    "\n",
    "Run the following cell to configure storage for your HyperParameter training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f017256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"iris\"\n",
    "bucket = next((bucket['Name'] for bucket in boto3.client('s3').list_buckets()['Buckets'] if bucket['Name'].startswith('lab-notebook-')), None)\n",
    "\n",
    "\n",
    "sess = sagemaker.Session(\n",
    "    default_bucket = bucket\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce10904",
   "metadata": {},
   "source": [
    "### Loading and Displaying the Data\n",
    "\n",
    "To load the `iris.csv` dataset, you will use the `pandas` library. The dataset contains four features: sepal length, sepal width, petal length, and petal width. The target column is the species of the iris flower.\n",
    "\n",
    "Run the following cell to load the dataset into a Pandas data frame and display the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62bdd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('./iris.csv', sep=',')\n",
    "pd.set_option('display.max_columns', 11)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 5)         # Keep the output on one page\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f657b0",
   "metadata": {},
   "source": [
    "### Preparing the Data for Training\n",
    "\n",
    "Typically, when training a machine learning model, you need to split the dataset into training and validation sets. The training set is used to train the model, and the validation set is used to evaluate the model's performance.\n",
    "\n",
    "As this dataset is small, no preprocessing is required apart from splitting the data. When working with larger and more complex datasets, you may need to perform additional preprocessing steps, such as normalizing the data or encoding categorical variables.\n",
    "\n",
    "It is conventional when performing classification tasks to reorder the data so that the target variable is the first column.\n",
    "\n",
    "Run the following cell to split the dataset into training and validation sets, re-order the columns, and display the first few rows of each dataset split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ebd73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the data into training (70%) and remaining data (30%)\n",
    "train_data, remaining_data = train_test_split(data, test_size=0.3, random_state=1729, stratify=model_data['Species'])\n",
    "\n",
    "# Then, split the remaining data into validation (20% of original data) and test (10% of original data)\n",
    "validation_data, test_data = train_test_split(remaining_data, test_size=1/3, random_state=1729, stratify=remaining_data['Species'])  # 1/3 of 30% is 10%\n",
    "\n",
    "# Reorder the columns to have 'Species' as the first column, but keep all other columns\n",
    "train_data[['Species'] + [col for col in train_data.columns if col != 'Species']].to_csv('train.csv', index=False, header=False)\n",
    "validation_data[['Species'] + [col for col in validation_data.columns if col != 'Species']].to_csv('validation.csv', index=False, header=False)\n",
    "test_data[['Species'] + [col for col in test_data.columns if col != 'Species']].to_csv('test.csv', index=False, header=False)\n",
    "\n",
    "# Display the first few rows of each dataset to verify the split\n",
    "print(\"Training Data:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(validation_data.head())\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd5e5a8",
   "metadata": {},
   "source": [
    "### Uploading the Data to Amazon S3\n",
    "\n",
    "Now that you have split the data into training and validation datasets, you have to make them available to the SageMaker service.\n",
    "\n",
    "The following cell creates Amazon S3 prefixes and URIs for the training and validation datasets. The datasets will be stored in different prefixes within the same bucket. You will use the S3 URIs when configuring the SageMaker HyperParameter Tuning job.\n",
    "\n",
    "Run the following cell to upload the training and validation datasets to the Amazon S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "213d696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1')\n",
    "\n",
    "train_s3_key = '{}/train'.format(prefix)\n",
    "validation_s3_key = '{}/validation'.format(prefix)\n",
    "\n",
    "s3_input_train = 's3://{}/{}'.format(bucket, train_s3_key)\n",
    "s3_input_validation ='s3://{}/{}/validation/'.format(bucket, validation_s3_key)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(train_s3_key, 'train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(validation_s3_key, 'validation.csv')).upload_file('validation.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b837e",
   "metadata": {},
   "source": [
    "### Configuring a HyperParameter Tuning Job\n",
    "\n",
    "The following cell creates a variable named `tuning_job_config` that contains the configuration for the SageMaker HyperParameter Tuning job.\n",
    "\n",
    "The `ParameterRanges` section specifies the parameters that are tunable. Each of these parameters can be changed to help prevent over fitting, and control the complexity of the model.\n",
    "\n",
    "When using Amazon HyperParameter Tuning with SageMaker, these parameters are modified iteratively to find the best combination of parameters for the model. Automating this tuning is often more efficient than manually tuning the parameters.\n",
    "\n",
    "The `ResourceLimits` section specifies the maximum number of training jobs that can be run in parallel and the maximum number of training jobs that can be run in total. In this lab, the numbers are small because the dataset is small, and the training jobs are quick to complete. In a non-lab environment, you may want to increase these numbers to speed up the tuning process.\n",
    "\n",
    "The `HyperParameterTuningJobObjective` section specifies the metric that you want to optimize. In this case, you are optimizing the validation loss. This is suitable for classification tasks where the goal is to minimize the loss. For other types of machine learning tasks, such as predicting a continuous value, you may want to optimize a different metric.\n",
    "\n",
    "Run the following cell to create a configuration for a HyperParameter Tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e426c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "        \"ContinuousParameterRanges\": [\n",
    "            {\n",
    "                \"Name\": \"eta\",\n",
    "                \"MinValue\": \"0.1\",\n",
    "                \"MaxValue\": \"0.5\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"min_child_weight\",\n",
    "                \"MinValue\": \"0\",\n",
    "                \"MaxValue\": \"120\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"subsample\",\n",
    "                \"MinValue\": \"0.5\",\n",
    "                \"MaxValue\": \"1\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"colsample_bytree\",\n",
    "                \"MinValue\": \"0.5\",\n",
    "                \"MaxValue\": \"1\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"gamma\",\n",
    "                \"MinValue\": \"0\",\n",
    "                \"MaxValue\": \"5\"\n",
    "            }\n",
    "        ],\n",
    "        \"IntegerParameterRanges\": [\n",
    "            {\n",
    "                \"Name\": \"max_depth\",\n",
    "                \"MinValue\": \"1\",\n",
    "                \"MaxValue\": \"10\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"ResourceLimits\": {\n",
    "        \"MaxNumberOfTrainingJobs\": 9,\n",
    "        \"MaxParallelTrainingJobs\": 3\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",  # Use Bayesian optimization for efficient tuning\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "        \"MetricName\": \"validation:mlogloss\",  # Suitable for multi-class classification\n",
    "        \"Type\": \"Minimize\"  # Minimize log loss for best accuracy\n",
    "    },\n",
    "    \"RandomSeed\": 123\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c466f21c",
   "metadata": {},
   "source": [
    "### Defining a HyperParameter Training Job\n",
    "\n",
    "The following cell creates a variable named `training_job_config` that contains the configuration for the SageMaker HyperParameter Tuning training job.\n",
    "\n",
    "You are using a managed algorithm container image provided by Amazon SageMaker called XGBoost (eXtreme Gradient Boosting). This is a popular algorithm for classification tasks. You can use your docker container image if you have a custom algorithm or custom dependencies.\n",
    "\n",
    "Notice that as well as specifying the `TrainingImage`, you also are providing a `RoleArn`. The role is used by SageMaker to access the training data and to store the model artifacts in Amazon S3.\n",
    "\n",
    "Run the following cell to define a HyperParameter Tuning training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1')\n",
    "\n",
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": training_image,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3_input_train\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3_input_validation\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}output\".format(bucket, prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.m5.large\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "        \"objective\": \"multi:softmax\",  # Changed to multi-class objective\n",
    "        \"num_class\": \"3\",  # Number of classes in the Iris dataset\n",
    "        \"eval_metric\": \"mlogloss\",  # Use multi-class log loss for evaluation\n",
    "        \"num_round\": \"100\"  # Number of boosting rounds\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 300\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d092fa0",
   "metadata": {},
   "source": [
    "To launch the HyperParameter Tuning job, you need to create a SageMaker client and call the `create_hyper_parameter_tuning_job` method. The Amazon SageMaker service expects to be called with a tuning job configuration and training job definition, as well as a name for the job.\n",
    "\n",
    "Run the following cell to create a SageMaker client and launch the HyperParameter Tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae94d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_name = \"iris-training-job\"\n",
    "\n",
    "smclient.create_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name,\n",
    "    HyperParameterTuningJobConfig=tuning_job_config,\n",
    "    TrainingJobDefinition=training_job_definition,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a41b081",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "training_image = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1')\n",
    "\n",
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": training_image,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3_input_train\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3_input_validation\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}output\".format(bucket, prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.m5.large\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "        \"objective\": \"multi:softmax\",  # Changed to multi-class objective\n",
    "        \"num_class\": \"3\",  # Number of classes in the Iris dataset\n",
    "        \"eval_metric\": \"mlogloss\",  # Use multi-class log loss for evaluation\n",
    "        \"num_round\": \"100\"  # Number of boosting rounds\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 300\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db23b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1')\n",
    "\n",
    "s3_input_train = 's3://{}/{}/train'.format(bucket, prefix)\n",
    "s3_input_validation ='s3://{}/{}/validation/'.format(bucket, prefix)\n",
    "\n",
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "        \"ContinuousParameterRanges\": [\n",
    "            {\n",
    "                \"Name\": \"eta\",\n",
    "                \"MinValue\": \"0.1\",\n",
    "                \"MaxValue\": \"0.5\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"min_child_weight\",\n",
    "                \"MinValue\": \"0\",\n",
    "                \"MaxValue\": \"120\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"subsample\",\n",
    "                \"MinValue\": \"0.5\",\n",
    "                \"MaxValue\": \"1\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"colsample_bytree\",\n",
    "                \"MinValue\": \"0.5\",\n",
    "                \"MaxValue\": \"1\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"gamma\",\n",
    "                \"MinValue\": \"0\",\n",
    "                \"MaxValue\": \"5\"\n",
    "            }\n",
    "        ],\n",
    "        \"IntegerParameterRanges\": [\n",
    "            {\n",
    "                \"Name\": \"max_depth\",\n",
    "                \"MinValue\": \"1\",\n",
    "                \"MaxValue\": \"10\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"ResourceLimits\": {\n",
    "        \"MaxNumberOfTrainingJobs\": 9,\n",
    "        \"MaxParallelTrainingJobs\": 3\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",  # Use Bayesian optimization for efficient tuning\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "        \"MetricName\": \"validation:mlogloss\",  # Suitable for multi-class classification\n",
    "        \"Type\": \"Minimize\"  # Minimize log loss for best accuracy\n",
    "    },\n",
    "    \"RandomSeed\": 123\n",
    "}\n",
    "\n",
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": training_image,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3_input_train\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3_input_validation\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}output\".format(bucket, prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,  # Reduced to 1 because the Iris dataset is small\n",
    "        \"InstanceType\": \"ml.m5.large\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "        \"objective\": \"multi:softmax\",  # Changed to multi-class objective\n",
    "        \"num_class\": \"3\",  # Number of classes in the Iris dataset\n",
    "        \"eval_metric\": \"mlogloss\",  # Use multi-class log loss for evaluation\n",
    "        \"num_round\": \"100\"  # Number of boosting rounds\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 300\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "tuning_job_name = \"iris-training-job-eikltf1\"\n",
    "smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n",
    "                                           HyperParameterTuningJobConfig = tuning_job_config,\n",
    "                                           TrainingJobDefinition = training_job_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
